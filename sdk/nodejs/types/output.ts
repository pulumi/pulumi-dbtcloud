// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

export interface BigquerySemanticLayerCredentialConfiguration {
    /**
     * The adapter version
     */
    adapterVersion: string;
    /**
     * The name of the configuration
     */
    name: string;
    /**
     * The ID of the project
     */
    projectId: number;
}

export interface BigquerySemanticLayerCredentialCredential {
    /**
     * The internal credential ID
     */
    credentialId: number;
    /**
     * Default dataset name
     */
    dataset: string;
    /**
     * The ID of this resource. Contains the project ID and the credential ID.
     */
    id: string;
    /**
     * Whether the BigQuery credential is active
     */
    isActive: boolean;
    /**
     * Number of threads to use
     */
    numThreads: number;
    /**
     * Project ID to create the BigQuery credential in
     */
    projectId: number;
}

export interface DatabricksSemanticLayerCredentialConfiguration {
    /**
     * The adapter version
     */
    adapterVersion: string;
    /**
     * The name of the configuration
     */
    name: string;
    /**
     * The ID of the project
     */
    projectId: number;
}

export interface DatabricksSemanticLayerCredentialCredential {
    /**
     * The type of the adapter (databricks or spark). Optional only when semantic*layer*credential is set to true; otherwise, this field is required.
     */
    adapterType: string;
    /**
     * The catalog where to create models (only for the databricks adapter)
     */
    catalog: string;
    /**
     * The system Databricks credential ID
     */
    credentialId: number;
    /**
     * The ID of this resource. Contains the project ID and the credential ID.
     */
    id: string;
    /**
     * Project ID to create the Databricks credential in
     */
    projectId: number;
    /**
     * The schema where to create models. Optional only when semantic*layer*credential is set to true; otherwise, this field is required.
     */
    schema: string;
    /**
     * This field indicates that the credential is used as part of the Semantic Layer configuration. It is used to create a Databricks credential for the Semantic Layer.
     */
    semanticLayerCredential: boolean;
    /**
     * Target name
     *
     * @deprecated This field is deprecated at the environment level (it was never possible to set it in the UI) and will be removed in a future release. Please remove it and set the target name at the job level or leverage environment variables.
     */
    targetName: string;
    /**
     * Token for Databricks user
     */
    token: string;
}

export interface GetEnvironmentsEnvironment {
    /**
     * A connection ID (used with Global Connections)
     */
    connectionId: number;
    /**
     * Credential ID for this environment. A credential is not required for development environments, as dbt Cloud defaults to the user's credentials, but deployment environments will have this.
     */
    credentialsId: number;
    /**
     * The custom branch name to use
     */
    customBranch: string;
    /**
     * Version number of dbt to use in this environment.
     */
    dbtVersion: string;
    /**
     * The type of deployment environment (currently 'production', 'staging' or empty)
     */
    deploymentType: string;
    /**
     * Whether model query history is on
     */
    enableModelQueryHistory: boolean;
    /**
     * The ID of the environment
     */
    environmentId: number;
    /**
     * The ID of the extended attributes applied
     */
    extendedAttributesId: number;
    /**
     * The name of the environment
     */
    name: string;
    /**
     * The project ID to which the environment belong
     */
    projectId: number;
    /**
     * The type of environment (must be either development or deployment)
     */
    type: string;
    /**
     * Whether to use a custom git branch in this environment
     */
    useCustomBranch: boolean;
}

export interface GetGlobalConnectionApacheSpark {
    /**
     * Auth
     */
    auth: string;
    /**
     * Spark cluster for the connection
     */
    cluster: string;
    /**
     * Connection retries. Default=0
     */
    connectRetries: number;
    /**
     * Connection time out in seconds. Default=10
     */
    connectTimeout: number;
    /**
     * Hostname of the connection
     */
    host: string;
    /**
     * Authentication method for the connection (http or thrift).
     */
    method: string;
    /**
     * Organization ID
     */
    organization: string;
    /**
     * Port for the connection. Default=443
     */
    port: number;
    /**
     * User
     */
    user: string;
}

export interface GetGlobalConnectionAthena {
    /**
     * Specify the database (data catalog) to build models into (lowercase only).
     */
    database: string;
    /**
     * Number of times to retry boto3 requests (e.g. deleting S3 files for materialized tables).
     */
    numBoto3Retries: number;
    /**
     * Number of times to retry iceberg commit queries to fix ICEBERG*COMMIT*ERROR.
     */
    numIcebergRetries: number;
    /**
     * Number of times to retry a failing query.
     */
    numRetries: number;
    /**
     * Interval in seconds to use for polling the status of query results in Athena.
     */
    pollInterval: number;
    /**
     * AWS region of your Athena instance.
     */
    regionName: string;
    /**
     * Prefix for storing tables, if different from the connection's S3 staging directory.
     */
    s3DataDir: string;
    /**
     * How to generate table paths in the S3 data directory.
     */
    s3DataNaming: string;
    /**
     * S3 location to store Athena query results and metadata.
     */
    s3StagingDir: string;
    /**
     * Prefix for storing temporary tables, if different from the connection's S3 data directory.
     */
    s3TmpTableDir: string;
    /**
     * Identifier of Athena Spark workgroup for running Python models.
     */
    sparkWorkGroup: string;
    /**
     * Identifier of Athena workgroup.
     */
    workGroup: string;
}

export interface GetGlobalConnectionBigquery {
    /**
     * OAuth Client ID
     */
    applicationId: string;
    /**
     * OAuth Client Secret
     */
    applicationSecret: string;
    /**
     * Auth Provider X509 Cert URL for the Service Account
     */
    authProviderX509CertUrl: string;
    /**
     * Auth URI for the Service Account
     */
    authUri: string;
    /**
     * Service Account email
     */
    clientEmail: string;
    /**
     * Client ID of the Service Account
     */
    clientId: string;
    /**
     * Client X509 Cert URL for the Service Account
     */
    clientX509CertUrl: string;
    /**
     * Dataproc cluster name for PySpark workloads
     */
    dataprocClusterName: string;
    /**
     * Google Cloud region for PySpark workloads on Dataproc
     */
    dataprocRegion: string;
    /**
     * Project to bill for query execution
     */
    executionProject: string;
    /**
     * The GCP project ID to use for the connection
     */
    gcpProjectId: string;
    /**
     * URI for a Google Cloud Storage bucket to host Python code executed via Datapro
     */
    gcsBucket: string;
    /**
     * Service Account to impersonate when running queries
     */
    impersonateServiceAccount: string;
    /**
     * Maximum timeout for the job creation step
     */
    jobCreationTimeoutSeconds: number;
    /**
     * Total number of seconds to wait while retrying the same query
     */
    jobRetryDeadlineSeconds: number;
    /**
     * Location to create new Datasets in
     */
    location: string;
    /**
     * Max number of bytes that can be billed for a given BigQuery query
     */
    maximumBytesBilled: number;
    /**
     * The priority with which to execute BigQuery queries (batch or interactive)
     */
    priority: string;
    /**
     * Private Key for the Service Account
     */
    privateKey: string;
    /**
     * Private Key ID for the Service Account
     */
    privateKeyId: string;
    /**
     * Number of retries for queries
     */
    retries: number;
    /**
     * OAuth scopes for the BigQuery connection
     */
    scopes: string[];
    /**
     * Timeout in seconds for queries
     */
    timeoutSeconds: number;
    /**
     * Token URI for the Service Account
     */
    tokenUri: string;
}

export interface GetGlobalConnectionDatabricks {
    /**
     * Catalog name if Unity Catalog is enabled in your Databricks workspace.
     */
    catalog: string;
    /**
     * Required to enable Databricks OAuth authentication for IDE developers.
     */
    clientId: string;
    /**
     * Required to enable Databricks OAuth authentication for IDE developers.
     */
    clientSecret: string;
    /**
     * The hostname of the Databricks cluster or SQL warehouse.
     */
    host: string;
    /**
     * The HTTP path of the Databricks cluster or SQL warehouse.
     */
    httpPath: string;
}

export interface GetGlobalConnectionFabric {
    /**
     * The database to connect to for this connection.
     */
    database: string;
    /**
     * The number of seconds used to establish a connection before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    loginTimeout: number;
    /**
     * The port to connect to for this connection. Default=1433
     */
    port: number;
    /**
     * The number of seconds used to wait for a query before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    queryTimeout: number;
    /**
     * The number of automatic times to retry a query before failing. Defaults to 1. Queries with syntax errors will not be retried. This setting can be used to overcome intermittent network issues.
     */
    retries: number;
    /**
     * The server hostname.
     */
    server: string;
}

export interface GetGlobalConnectionPostgres {
    /**
     * The database name for this connection.
     */
    dbname: string;
    /**
     * The hostname of the database.
     */
    hostname: string;
    /**
     * The port to connect to for this connection. Default=5432
     */
    port: number;
    /**
     * PostgreSQL SSH Tunnel configuration
     */
    sshTunnel: outputs.GetGlobalConnectionPostgresSshTunnel;
}

export interface GetGlobalConnectionPostgresSshTunnel {
    /**
     * The hostname for the SSH tunnel.
     */
    hostname: string;
    /**
     * The ID of the SSH tunnel connection.
     */
    id: number;
    /**
     * The HTTP port for the SSH tunnel.
     */
    port: number;
    /**
     * The SSH public key generated to allow connecting via SSH tunnel.
     */
    publicKey: string;
    /**
     * The username to use for the SSH tunnel.
     */
    username: string;
}

export interface GetGlobalConnectionRedshift {
    /**
     * The database name for this connection.
     */
    dbname: string;
    /**
     * The hostname of the data warehouse.
     */
    hostname: string;
    /**
     * The port to connect to for this connection. Default=5432
     */
    port: number;
    /**
     * Redshift SSH Tunnel configuration
     */
    sshTunnel: outputs.GetGlobalConnectionRedshiftSshTunnel;
}

export interface GetGlobalConnectionRedshiftSshTunnel {
    /**
     * The hostname for the SSH tunnel.
     */
    hostname: string;
    /**
     * The ID of the SSH tunnel connection.
     */
    id: number;
    /**
     * The HTTP port for the SSH tunnel.
     */
    port: number;
    /**
     * The SSH public key generated to allow connecting via SSH tunnel.
     */
    publicKey: string;
    /**
     * The username to use for the SSH tunnel.
     */
    username: string;
}

export interface GetGlobalConnectionSnowflake {
    /**
     * The Snowflake account name
     */
    account: string;
    /**
     * Whether to allow Snowflake OAuth for the connection. If true, the `oauthClientId` and `oauthClientSecret` fields must be set
     */
    allowSso: boolean;
    /**
     * If true, the snowflake client will keep connections for longer than the default 4 hours. This is helpful when particularly long-running queries are executing (> 4 hours)
     */
    clientSessionKeepAlive: boolean;
    /**
     * The default database for the connection
     */
    database: string;
    /**
     * OAuth Client ID. Required to allow OAuth between dbt Cloud and Snowflake
     */
    oauthClientId: string;
    /**
     * OAuth Client Secret. Required to allow OAuth between dbt Cloud and Snowflake
     */
    oauthClientSecret: string;
    /**
     * The Snowflake role to use when running queries on the connection
     */
    role: string;
    /**
     * The default Snowflake Warehouse to use for the connection
     */
    warehouse: string;
}

export interface GetGlobalConnectionStarburst {
    /**
     * The hostname of the account to connect to.
     */
    host: string;
    /**
     * The authentication method. Only LDAP for now.
     */
    method: string;
    /**
     * The port to connect to for this connection. Default=443
     */
    port: number;
}

export interface GetGlobalConnectionSynapse {
    /**
     * The database to connect to for this connection.
     */
    database: string;
    /**
     * The server hostname.
     */
    host: string;
    /**
     * The number of seconds used to establish a connection before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    loginTimeout: number;
    /**
     * The port to connect to for this connection. Default=1433
     */
    port: number;
    /**
     * The number of seconds used to wait for a query before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    queryTimeout: number;
    /**
     * The number of automatic times to retry a query before failing. Defaults to 1. Queries with syntax errors will not be retried. This setting can be used to overcome intermittent network issues.
     */
    retries: number;
}

export interface GetGlobalConnectionTeradata {
    /**
     * The hostname of the database.
     */
    host: string;
    /**
     * The port to connect to for this connection. Default=1025
     */
    port: string;
    /**
     * The number of seconds used to establish a connection before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    requestTimeout: number;
    /**
     * The number of automatic times to retry a query before failing. Defaults to 1. Queries with syntax errors will not be retried. This setting can be used to overcome intermittent network issues.
     */
    retries: number;
    /**
     * The transaction mode to use for the connection.
     */
    tmode: string;
}

export interface GetGlobalConnectionsConnection {
    /**
     * Type of adapter used for the connection
     */
    adapterVersion: string;
    /**
     * When the connection was created
     */
    createdAt: string;
    /**
     * Number of environments using this connection
     */
    environmentCount: number;
    /**
     * Connection Identifier
     */
    id: number;
    isSshTunnelEnabled: boolean;
    /**
     * Connection name
     */
    name: string;
    oauthConfigurationId: number;
    /**
     * Private Link Endpoint ID.
     */
    privateLinkEndpointId: string;
    /**
     * When the connection was updated
     */
    updatedAt: string;
}

export interface GetGroupGroupPermission {
    /**
     * Whether access should be provided for all projects or not.
     */
    allProjects: boolean;
    /**
     * Set of permissions to apply. The permissions allowed are the same as the ones for the `dbtcloud.Group` resource.
     */
    permissionSet: string;
    /**
     * Project ID to apply this permission to for this group.
     */
    projectId: number;
    /**
     * What types of environments to apply Write permissions to.
     */
    writableEnvironmentCategories: string[];
}

export interface GetGroupUsersUser {
    /**
     * Email of the user
     */
    email: string;
    /**
     * ID of the user
     */
    id: number;
}

export interface GetGroupsGroup {
    /**
     * Whether the group will be assigned by default to users
     */
    assignByDefault: boolean;
    /**
     * The ID of the group
     */
    id: number;
    /**
     * Group name
     */
    name: string;
    /**
     * Whether the group is managed by SCIM
     */
    scimManaged: boolean;
    /**
     * SSO mapping group names for this group
     */
    ssoMappingGroups: string[];
    /**
     * The state of the group (1=active, 2=deleted)
     */
    state: number;
}

export interface GetJobEnvironment {
    /**
     * Type of deployment environment: staging, production
     */
    deploymentType: string;
    /**
     * ID of the environment
     */
    id: number;
    /**
     * Name of the environment
     */
    name: string;
    projectId: number;
    /**
     * Environment type: development or deployment
     */
    type: string;
}

export interface GetJobExecution {
    /**
     * The number of seconds before the job times out
     */
    timeoutSeconds: number;
}

export interface GetJobJobCompletionTriggerCondition {
    /**
     * The ID of the job that would trigger this job after completion.
     */
    jobId: number;
    /**
     * The ID of the project where the trigger job is running in.
     */
    projectId: number;
    /**
     * List of statuses to trigger the job on.
     */
    statuses: string[];
}

export interface GetJobSchedule {
    /**
     * The cron schedule for the job. Only used if triggers.schedule is true
     */
    cron: string;
}

export interface GetJobSettings {
    /**
     * Value for `target.name` in the Jinja context
     */
    targetName: string;
    /**
     * Number of threads to run dbt with
     */
    threads: number;
}

export interface GetJobTriggers {
    /**
     * Whether the job runs automatically on PR creation
     */
    gitProviderWebhook: boolean;
    /**
     * Whether the job runs automatically on PR creation
     */
    githubWebhook: boolean;
    /**
     * Whether the job runs automatically once a PR is merged
     */
    onMerge: boolean;
    /**
     * Whether the job runs on a schedule
     */
    schedule: boolean;
}

export interface GetJobsJob {
    /**
     * The version of dbt used for the job. If not set, the environment version will be used.
     */
    dbtVersion: string;
    /**
     * The ID of the environment this job defers to
     */
    deferringEnvironmentId: number;
    /**
     * [Deprectated - Deferral is now set at the environment level] The ID of the job definition this job defers to
     *
     * @deprecated Deferral is now set at the environment level
     */
    deferringJobDefinitionId: number;
    /**
     * The description of the job
     */
    description: string;
    /**
     * Details of the environment the job is running in
     */
    environment: outputs.GetJobsJobEnvironment;
    /**
     * The ID of environment
     */
    environmentId: number;
    /**
     * The list of steps to run in the job
     */
    executeSteps: string[];
    execution: outputs.GetJobsJobExecution;
    /**
     * Whether the job generate docs
     */
    generateDocs: boolean;
    /**
     * The ID of the job
     */
    id: number;
    /**
     * Whether the job is triggered by the completion of another job
     */
    jobCompletionTriggerCondition: outputs.GetJobsJobJobCompletionTriggerCondition;
    /**
     * The ID of the job
     */
    jobId: number;
    /**
     * The type of job (e.g. CI, scheduled)
     */
    jobType: string;
    /**
     * The name of the job
     */
    name: string;
    /**
     * The ID of the project
     */
    projectId: number;
    /**
     * Whether the job should compare data changes introduced by the code change in the PR
     */
    runCompareChanges: boolean;
    /**
     * Whether the job test source freshness
     */
    runGenerateSources: boolean;
    schedule: outputs.GetJobsJobSchedule;
    settings: outputs.GetJobsJobSettings;
    /**
     * [Deprectated - Moved to execution.timeout_seconds] Number of seconds before the job times out
     *
     * @deprecated Moved to execution.timeout_seconds
     */
    timeoutSeconds: number;
    triggers: outputs.GetJobsJobTriggers;
    /**
     * Whether the CI job should be automatically triggered on draft PRs
     */
    triggersOnDraftPr: boolean;
}

export interface GetJobsJobEnvironment {
    /**
     * Type of deployment environment: staging, production
     */
    deploymentType: string;
    /**
     * ID of the environment
     */
    id: number;
    /**
     * Name of the environment
     */
    name: string;
    projectId: number;
    /**
     * Environment type: development or deployment
     */
    type: string;
}

export interface GetJobsJobExecution {
    /**
     * The number of seconds before the job times out
     */
    timeoutSeconds: number;
}

export interface GetJobsJobJobCompletionTriggerCondition {
    condition: outputs.GetJobsJobJobCompletionTriggerConditionCondition;
}

export interface GetJobsJobJobCompletionTriggerConditionCondition {
    jobId: number;
    projectId: number;
    statuses: string[];
}

export interface GetJobsJobSchedule {
    /**
     * The cron schedule for the job. Only used if triggers.schedule is true
     */
    cron: string;
}

export interface GetJobsJobSettings {
    /**
     * Value for `target.name` in the Jinja context
     */
    targetName: string;
    /**
     * Number of threads to run dbt with
     */
    threads: number;
}

export interface GetJobsJobTriggers {
    /**
     * Whether the job runs automatically on PR creation
     */
    gitProviderWebhook: boolean;
    /**
     * Whether the job runs automatically on PR creation
     */
    githubWebhook: boolean;
    /**
     * Whether the job runs automatically once a PR is merged
     */
    onMerge: boolean;
    /**
     * Whether the job runs on a schedule
     */
    schedule: boolean;
}

export interface GetPrivatelinkEndpointsEndpoint {
    /**
     * CIDR range of the PrivateLink Endpoint
     */
    cidrRange: string;
    /**
     * The internal ID of the PrivateLink Endpoint
     */
    id: string;
    /**
     * Given descriptive name for the PrivateLink Endpoint
     */
    name: string;
    /**
     * URL of the PrivateLink Endpoint
     */
    privateLinkEndpointUrl: string;
    /**
     * Type of the PrivateLink Endpoint
     */
    type: string;
}

export interface GetProjectProjectConnection {
    /**
     * Version of the adapter for the connection. Will tell what connection type it is
     */
    adapterVersion: string;
    /**
     * Connection ID
     */
    id: number;
    /**
     * Connection name
     */
    name: string;
}

export interface GetProjectRepository {
    /**
     * Repository ID
     */
    id: number;
    /**
     * URL template for PRs
     */
    pullRequestUrlTemplate: string;
    /**
     * URL of the git repo remote
     */
    remoteUrl: string;
}

export interface GetProjectsProject {
    /**
     * When the project was created
     */
    createdAt: string;
    /**
     * Subdirectory for the dbt project inside the git repo
     */
    dbtProjectSubdirectory: string;
    /**
     * Project description
     */
    description: string;
    /**
     * Project ID
     */
    id: number;
    /**
     * Project name
     */
    name: string;
    /**
     * Details for the connection linked to the project
     */
    projectConnection: outputs.GetProjectsProjectProjectConnection;
    /**
     * Details for the repository linked to the project
     */
    repository: outputs.GetProjectsProjectRepository;
    /**
     * Semantic layer config ID
     */
    semanticLayerConfigId: number;
    /**
     * The type of dbt project (default or hybrid)
     */
    type: number;
    /**
     * When the project was last updated
     */
    updatedAt: string;
}

export interface GetProjectsProjectProjectConnection {
    /**
     * Version of the adapter for the connection. Will tell what connection type it is
     */
    adapterVersion: string;
    /**
     * Connection ID
     */
    id: number;
    /**
     * Connection name
     */
    name: string;
}

export interface GetProjectsProjectRepository {
    /**
     * Repository ID
     */
    id: number;
    /**
     * URL template for PRs
     */
    pullRequestUrlTemplate: string;
    /**
     * URL of the git repo remote
     */
    remoteUrl: string;
}

export interface GetRunsFilter {
    /**
     * The ID of the environment
     */
    environmentId?: number;
    /**
     * The ID of the job definition
     */
    jobDefinitionId?: number;
    /**
     * The limit of the runs
     */
    limit?: number;
    /**
     * The ID of the project
     */
    projectId?: number;
    /**
     * The ID of the pull request
     */
    pullRequestId?: number;
    /**
     * The status of the run
     */
    status?: number;
    /**
     * The status of the run
     */
    statusIn?: string;
    /**
     * The ID of the trigger
     */
    triggerId?: number;
}

export interface GetRunsRun {
    /**
     * The ID of the account
     */
    accountId: number;
    /**
     * The cause of the run
     */
    cause: string;
    /**
     * The branch of the commit
     */
    gitBranch: string;
    /**
     * The SHA of the commit
     */
    gitSha: string;
    /**
     * The ID of the pull request
     */
    githubPullRequestId: string;
    /**
     * The ID of the run
     */
    id: number;
    /**
     * The ID of the job
     */
    jobId: number;
    /**
     * The schema override
     */
    schemaOverride: string;
}

export interface GetServiceTokenServiceTokenPermission {
    /**
     * Whether or not to apply this permission to all projects for this service token
     */
    allProjects: boolean;
    /**
     * Set of permissions to apply
     */
    permissionSet: string;
    /**
     * Project ID to apply this permission to for this service token
     */
    projectId: number;
    /**
     * What types of environments to apply Write permissions to.
     * Even if Write access is restricted to some environment types, the permission set will have Read access to all environments.
     * The values allowed are `all`, `development`, `staging`, `production` and `other`.
     * Not setting a value is the same as selecting `all`.
     * Not all permission sets support environment level write settings, only `analyst`, `databaseAdmin`, `developer`, `gitAdmin` and `teamAdmin`.
     */
    writableEnvironmentCategories: string[];
}

export interface GetUsersUser {
    /**
     * Email for the user
     */
    email: string;
    /**
     * ID of the user
     */
    id: number;
}

export interface GlobalConnectionApacheSpark {
    /**
     * Auth
     */
    auth?: string;
    /**
     * Spark cluster for the connection
     */
    cluster: string;
    /**
     * Connection retries. Default=0
     */
    connectRetries: number;
    /**
     * Connection time out in seconds. Default=10
     */
    connectTimeout: number;
    /**
     * Hostname of the connection
     */
    host: string;
    /**
     * Authentication method for the connection (http or thrift).
     */
    method: string;
    /**
     * Organization ID
     */
    organization?: string;
    /**
     * Port for the connection. Default=443
     */
    port: number;
    /**
     * User
     */
    user?: string;
}

export interface GlobalConnectionAthena {
    /**
     * Specify the database (data catalog) to build models into (lowercase only).
     */
    database: string;
    /**
     * Number of times to retry boto3 requests (e.g. deleting S3 files for materialized tables).
     */
    numBoto3Retries?: number;
    /**
     * Number of times to retry iceberg commit queries to fix ICEBERG*COMMIT*ERROR.
     */
    numIcebergRetries?: number;
    /**
     * Number of times to retry a failing query.
     */
    numRetries?: number;
    /**
     * Interval in seconds to use for polling the status of query results in Athena.
     */
    pollInterval?: number;
    /**
     * AWS region of your Athena instance.
     */
    regionName: string;
    /**
     * Prefix for storing tables, if different from the connection's S3 staging directory.
     */
    s3DataDir?: string;
    /**
     * How to generate table paths in the S3 data directory.
     */
    s3DataNaming?: string;
    /**
     * S3 location to store Athena query results and metadata.
     */
    s3StagingDir: string;
    /**
     * Prefix for storing temporary tables, if different from the connection's S3 data directory.
     */
    s3TmpTableDir?: string;
    /**
     * Identifier of Athena Spark workgroup for running Python models.
     */
    sparkWorkGroup?: string;
    /**
     * Identifier of Athena workgroup.
     */
    workGroup?: string;
}

export interface GlobalConnectionBigquery {
    /**
     * OAuth Client ID
     */
    applicationId?: string;
    /**
     * OAuth Client Secret
     */
    applicationSecret?: string;
    /**
     * Auth Provider X509 Cert URL for the Service Account
     */
    authProviderX509CertUrl: string;
    /**
     * Auth URI for the Service Account
     */
    authUri: string;
    /**
     * Service Account email
     */
    clientEmail: string;
    /**
     * Client ID of the Service Account
     */
    clientId: string;
    /**
     * Client X509 Cert URL for the Service Account
     */
    clientX509CertUrl: string;
    /**
     * Dataproc cluster name for PySpark workloads
     */
    dataprocClusterName?: string;
    /**
     * Google Cloud region for PySpark workloads on Dataproc
     */
    dataprocRegion?: string;
    /**
     * Project to bill for query execution
     */
    executionProject?: string;
    /**
     * The GCP project ID to use for the connection
     */
    gcpProjectId: string;
    /**
     * URI for a Google Cloud Storage bucket to host Python code executed via Datapro
     */
    gcsBucket?: string;
    /**
     * Service Account to impersonate when running queries
     */
    impersonateServiceAccount?: string;
    /**
     * Maximum timeout for the job creation step
     */
    jobCreationTimeoutSeconds?: number;
    /**
     * Timeout in seconds for job execution, to be used for the bigqueryV1 adapter
     */
    jobExecutionTimeoutSeconds?: number;
    /**
     * Total number of seconds to wait while retrying the same query
     */
    jobRetryDeadlineSeconds?: number;
    /**
     * Location to create new Datasets in
     */
    location?: string;
    /**
     * Max number of bytes that can be billed for a given BigQuery query
     */
    maximumBytesBilled?: number;
    /**
     * The priority with which to execute BigQuery queries (batch or interactive)
     */
    priority?: string;
    /**
     * Private Key for the Service Account
     */
    privateKey: string;
    /**
     * Private Key ID for the Service Account
     */
    privateKeyId: string;
    /**
     * Number of retries for queries
     */
    retries: number;
    /**
     * OAuth scopes for the BigQuery connection
     */
    scopes: string[];
    /**
     * Timeout in seconds for queries, to be used ONLY for the bigqueryV0 adapter
     */
    timeoutSeconds: number;
    /**
     * Token URI for the Service Account
     */
    tokenUri: string;
    /**
     * Whether to use the latest bigqueryV1 adapter (use this for BQ WIF). If true, the `jobExecutionTimeoutSeconds` field will be used. Warning! changing the adapter version (from legacy to latest or vice versa) is not supported.
     */
    useLatestAdapter?: boolean;
}

export interface GlobalConnectionDatabricks {
    /**
     * Catalog name if Unity Catalog is enabled in your Databricks workspace.
     */
    catalog?: string;
    /**
     * Required to enable Databricks OAuth authentication for IDE developers.
     */
    clientId?: string;
    /**
     * Required to enable Databricks OAuth authentication for IDE developers.
     */
    clientSecret?: string;
    /**
     * The hostname of the Databricks cluster or SQL warehouse.
     */
    host: string;
    /**
     * The HTTP path of the Databricks cluster or SQL warehouse.
     */
    httpPath: string;
}

export interface GlobalConnectionFabric {
    /**
     * The database to connect to for this connection.
     */
    database: string;
    /**
     * The number of seconds used to establish a connection before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    loginTimeout: number;
    /**
     * The port to connect to for this connection. Default=1433
     */
    port: number;
    /**
     * The number of seconds used to wait for a query before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    queryTimeout: number;
    /**
     * The number of automatic times to retry a query before failing. Defaults to 1. Queries with syntax errors will not be retried. This setting can be used to overcome intermittent network issues.
     */
    retries: number;
    /**
     * The server hostname.
     */
    server: string;
}

export interface GlobalConnectionPostgres {
    /**
     * The database name for this connection.
     */
    dbname: string;
    /**
     * The hostname of the database.
     */
    hostname: string;
    /**
     * The port to connect to for this connection. Default=5432
     */
    port: number;
    /**
     * PostgreSQL SSH Tunnel configuration
     */
    sshTunnel?: outputs.GlobalConnectionPostgresSshTunnel;
}

export interface GlobalConnectionPostgresSshTunnel {
    /**
     * The hostname for the SSH tunnel.
     */
    hostname: string;
    /**
     * The ID of the SSH tunnel connection.
     */
    id: number;
    /**
     * The HTTP port for the SSH tunnel.
     */
    port: number;
    /**
     * The SSH public key generated to allow connecting via SSH tunnel.
     */
    publicKey: string;
    /**
     * The username to use for the SSH tunnel.
     */
    username: string;
}

export interface GlobalConnectionRedshift {
    /**
     * The database name for this connection.
     */
    dbname: string;
    /**
     * The hostname of the data warehouse.
     */
    hostname: string;
    /**
     * The port to connect to for this connection. Default=5432
     */
    port: number;
    /**
     * Redshift SSH Tunnel configuration
     */
    sshTunnel?: outputs.GlobalConnectionRedshiftSshTunnel;
}

export interface GlobalConnectionRedshiftSshTunnel {
    /**
     * The hostname for the SSH tunnel.
     */
    hostname: string;
    /**
     * The ID of the SSH tunnel connection.
     */
    id: number;
    /**
     * The HTTP port for the SSH tunnel.
     */
    port: number;
    /**
     * The SSH public key generated to allow connecting via SSH tunnel.
     */
    publicKey: string;
    /**
     * The username to use for the SSH tunnel.
     */
    username: string;
}

export interface GlobalConnectionSnowflake {
    /**
     * The Snowflake account name
     */
    account: string;
    /**
     * Whether to allow Snowflake OAuth for the connection. If true, the `oauthClientId` and `oauthClientSecret` fields must be set
     */
    allowSso: boolean;
    /**
     * If true, the snowflake client will keep connections for longer than the default 4 hours. This is helpful when particularly long-running queries are executing (> 4 hours)
     */
    clientSessionKeepAlive: boolean;
    /**
     * The default database for the connection
     */
    database: string;
    /**
     * OAuth Client ID. Required to allow OAuth between dbt Cloud and Snowflake
     */
    oauthClientId?: string;
    /**
     * OAuth Client Secret. Required to allow OAuth between dbt Cloud and Snowflake
     */
    oauthClientSecret?: string;
    /**
     * The Snowflake role to use when running queries on the connection
     */
    role?: string;
    /**
     * The default Snowflake Warehouse to use for the connection
     */
    warehouse: string;
}

export interface GlobalConnectionStarburst {
    /**
     * The hostname of the account to connect to.
     */
    host: string;
    /**
     * The authentication method. Only LDAP for now.
     */
    method: string;
    /**
     * The port to connect to for this connection. Default=443
     */
    port: number;
}

export interface GlobalConnectionSynapse {
    /**
     * The database to connect to for this connection.
     */
    database: string;
    /**
     * The server hostname.
     */
    host: string;
    /**
     * The number of seconds used to establish a connection before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    loginTimeout: number;
    /**
     * The port to connect to for this connection. Default=1433
     */
    port: number;
    /**
     * The number of seconds used to wait for a query before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    queryTimeout: number;
    /**
     * The number of automatic times to retry a query before failing. Defaults to 1. Queries with syntax errors will not be retried. This setting can be used to overcome intermittent network issues.
     */
    retries: number;
}

export interface GlobalConnectionTeradata {
    /**
     * The hostname of the database.
     */
    host: string;
    /**
     * The port to connect to for this connection. Default=1025
     */
    port: string;
    /**
     * The number of seconds used to establish a connection before failing. Defaults to 0, which means that the timeout is disabled or uses the default system settings.
     */
    requestTimeout: number;
    /**
     * The number of automatic times to retry a query before failing. Defaults to 1. Queries with syntax errors will not be retried. This setting can be used to overcome intermittent network issues.
     */
    retries: number;
    /**
     * The transaction mode to use for the connection.
     */
    tmode: string;
}

export interface GroupGroupPermission {
    /**
     * Whether access should be provided for all projects or not.
     */
    allProjects: boolean;
    /**
     * Set of permissions to apply. The permissions allowed are the same as the ones for the `dbtcloud.Group` resource.
     */
    permissionSet: string;
    /**
     * Project ID to apply this permission to for this group.
     */
    projectId?: number;
    /**
     * What types of environments to apply Write permissions to.
     * Even if Write access is restricted to some environment types, the permission set will have Read access to all environments.
     * The values allowed are `all`, `development`, `staging`, `production` and `other`.
     * Not setting a value is the same as selecting `all`.
     * Not all permission sets support environment level write settings, only `analyst`, `databaseAdmin`, `developer`, `gitAdmin` and `teamAdmin`.
     */
    writableEnvironmentCategories: string[];
}

export interface GroupPartialPermissionsGroupPermission {
    /**
     * Whether access should be provided for all projects or not.
     */
    allProjects: boolean;
    /**
     * Set of permissions to apply. The permissions allowed are the same as the ones for the `dbtcloud.Group` resource.
     */
    permissionSet: string;
    /**
     * Project ID to apply this permission to for this group.
     */
    projectId?: number;
    /**
     * What types of environments to apply Write permissions to.
     * Even if Write access is restricted to some environment types, the permission set will have Read access to all environments.
     * The values allowed are `all`, `development`, `staging`, `production` and `other`.
     * Not setting a value is the same as selecting `all`.
     * Not all permission sets support environment level write settings, only `analyst`, `databaseAdmin`, `developer`, `gitAdmin` and `teamAdmin`.
     */
    writableEnvironmentCategories?: string[];
}

export interface IpRestrictionsRuleCidr {
    /**
     * IP CIDR range (can be IPv4 or IPv6)
     */
    cidr: string;
    /**
     * IPv6 CIDR range (read-only)
     */
    cidrIpv6: string;
    /**
     * ID of the CIDR range
     */
    id: number;
    /**
     * ID of the IP restriction rule
     */
    ipRestrictionRuleId: number;
}

export interface JobJobCompletionTriggerCondition {
    /**
     * The ID of the job that would trigger this job after completion.
     */
    jobId: number;
    /**
     * The ID of the project where the trigger job is running in.
     */
    projectId: number;
    /**
     * List of statuses to trigger the job on. Possible values are `success`, `error` and `canceled`.
     */
    statuses: string[];
}

export interface JobTriggers {
    /**
     * Whether the job runs automatically on PR creation
     */
    gitProviderWebhook: boolean;
    /**
     * Whether the job runs automatically on PR creation
     */
    githubWebhook: boolean;
    /**
     * Whether the job runs automatically once a PR is merged
     */
    onMerge: boolean;
    /**
     * Whether the job runs on a schedule
     */
    schedule: boolean;
}

export interface PostgresSemanticLayerCredentialConfiguration {
    /**
     * The adapter version
     */
    adapterVersion: string;
    /**
     * The name of the configuration
     */
    name: string;
    /**
     * The ID of the project
     */
    projectId: number;
}

export interface PostgresSemanticLayerCredentialCredential {
    /**
     * The system Postgres/Redshift/AlloyDB credential ID.
     */
    credentialId: number;
    /**
     * Default schema name. Optional only when semantic*layer*credential is set to true; otherwise, this field is required.
     */
    defaultSchema: string;
    /**
     * The ID of this resource. Contains the project ID and the credential ID.
     */
    id: string;
    /**
     * Whether the Postgres/Redshift/AlloyDB credential is active
     */
    isActive: boolean;
    /**
     * Number of threads to use (required for Redshift)
     */
    numThreads: number;
    /**
     * Password for Postgres/Redshift/AlloyDB
     */
    password?: string;
    /**
     * Project ID to create the Postgres/Redshift/AlloyDB credential in.
     */
    projectId: number;
    /**
     * This field indicates that the credential is used as part of the Semantic Layer configuration. It is used to create a Postgres credential for the Semantic Layer.
     */
    semanticLayerCredential: boolean;
    /**
     * Default schema name
     */
    targetName: string;
    /**
     * Type of connection. One of (postgres/redshift). Use postgres for alloydb connections. Optional only when semantic*layer*credential is set to true; otherwise, this field is required.
     */
    type: string;
    /**
     * Username for Postgres/Redshift/AlloyDB
     */
    username: string;
}

export interface RedshiftSemanticLayerCredentialConfiguration {
    /**
     * The adapter version
     */
    adapterVersion: string;
    /**
     * The name of the configuration
     */
    name: string;
    /**
     * The ID of the project
     */
    projectId: number;
}

export interface RedshiftSemanticLayerCredentialCredential {
    /**
     * The internal credential ID
     */
    credentialId: number;
    /**
     * Default schema name
     */
    defaultSchema: string;
    /**
     * The ID of this resource. Contains the project ID and the credential ID.
     */
    id: string;
    /**
     * Whether the Redshift credential is active
     */
    isActive: boolean;
    /**
     * Number of threads to use
     */
    numThreads: number;
    /**
     * The password for the Redshift account
     */
    password: string;
    /**
     * Project ID to create the Redshift credential in
     */
    projectId: number;
    /**
     * The username for the Redshift account.
     */
    username: string;
}

export interface ScimGroupPermissionsPermission {
    /**
     * Whether access should be provided for all projects or not.
     */
    allProjects: boolean;
    /**
     * Set of permissions to apply. The permissions allowed are the same as the ones for the `dbtcloud.Group` resource.
     */
    permissionSet: string;
    /**
     * Project ID to apply this permission to for this group.
     */
    projectId?: number;
    /**
     * What types of environments to apply Write permissions to.
     * Even if Write access is restricted to some environment types, the permission set will have Read access to all environments.
     * The values allowed are `all`, `development`, `staging`, `production` and `other`.
     * Not setting a value is the same as selecting `all`.
     * Not all permission sets support environment level write settings, only `analyst`, `databaseAdmin`, `developer`, `gitAdmin` and `teamAdmin`.
     */
    writableEnvironmentCategories?: string[];
}

export interface ServiceTokenServiceTokenPermission {
    /**
     * Whether or not to apply this permission to all projects for this service token
     */
    allProjects: boolean;
    /**
     * Set of permissions to apply
     */
    permissionSet: string;
    /**
     * Project ID to apply this permission to for this service token
     */
    projectId?: number;
    /**
     * What types of environments to apply Write permissions to.
     * Even if Write access is restricted to some environment types, the permission set will have Read access to all environments.
     * The values allowed are `all`, `development`, `staging`, `production` and `other`.
     * Not setting a value is the same as selecting `all`.
     * Not all permission sets support environment level write settings, only `analyst`, `databaseAdmin`, `developer`, `gitAdmin` and `teamAdmin`.
     */
    writableEnvironmentCategories: string[];
}

export interface SnowflakeSemanticLayerCredentialConfiguration {
    /**
     * The adapter version
     */
    adapterVersion: string;
    /**
     * The name of the configuration
     */
    name: string;
    /**
     * The ID of the project
     */
    projectId: number;
}

export interface SnowflakeSemanticLayerCredentialCredential {
    /**
     * The type of Snowflake credential ('password' or 'keypair')
     */
    authType: string;
    /**
     * The internal credential ID
     */
    credentialId: number;
    /**
     * The catalog to connect use
     */
    database?: string;
    /**
     * The ID of this resource. Contains the project ID and the credential ID.
     */
    id: string;
    /**
     * Whether the Snowflake credential is active
     */
    isActive: boolean;
    /**
     * Number of threads to use
     */
    numThreads: number;
    /**
     * The password for the Snowflake account
     */
    password: string;
    /**
     * The private key for the Snowflake account
     */
    privateKey: string;
    /**
     * The passphrase for the private key
     */
    privateKeyPassphrase: string;
    /**
     * Project ID to create the Snowflake credential in
     */
    projectId: number;
    /**
     * The role to assume
     */
    role?: string;
    /**
     * The schema where to create models. This is an optional field ONLY if the credential is used for Semantic Layer configuration, otherwise it is required.
     */
    schema: string;
    /**
     * This field indicates that the credential is used as part of the Semantic Layer configuration. It is used to create a Snowflake credential for the Semantic Layer.
     */
    semanticLayerCredential: boolean;
    /**
     * The username for the Snowflake account. This is an optional field ONLY if the credential is used for Semantic Layer configuration, otherwise it is required.
     */
    user: string;
    /**
     * The warehouse to use
     */
    warehouse?: string;
}

